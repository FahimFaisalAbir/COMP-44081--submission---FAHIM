
Assumptions-
Stationarity: We assume that the series is stationary, so it has consistent statistical properties over time. 
Linearity: Based on the plots, we assume linear relationships between variables (both for redemption count and sales count)
Data Quality: Based on the website’s data quality report we assume that the data is accurate, consistent data without significant missing values.
Stable External Environment: We also excluded other external factors so that redemption / sales are not influenced by other factors
Independent Residuals: Uncorrelated forecast errors.

I inspected the dataset and performed exploratory data analysis to identify important features. Initially, I performed feature engineering to add insightful, relevant features (year, day of week, day of year, day of month, week of year) that might help in prediction task. I added calendar features: month, quarter, day-of-week, and day-of-year to improve predictability of redemption / sales count.  My goal is to forecast daily redemption count accurately while handling uncertainty, seasonality and weekly trends. I improved feature engineering and tried to reduce uncertainty by introducing lags In different interval via machine learning models to capture generalized trend and broader seasonal impacts. I created relevant lagged features for sales and redemption, moving averages, and seasonal indicators. I explored the data. There are 0 values in redemption count and sales count which creates problem while using Mean absolute percentages error and so I decided to use refined score and MSE , MAE instead MAPE. There are lot of seasonality in the data which impacts the general trend .

I loaded data and converted timestamps into daily aggregates by summing up hourly values, and I identified entries with zero redemption counts which can result in skewed MAPE. I explored other frequence (e.g. – day , week, hours interval) and day worked best. Instead of filtering zero sales/ redemption in initial phase I modified the score calculation methods to tackle null values so that it can deal real life cases with zero redemptions. Then I visualized data for patterns, trends, seasonality, and anomalies. I also computed autocorrelations (ACF, PACF) to identify relevant lag structures before using statistical models. The PACF plot shows direct correlation at each lag interval, aiding in identifying autoregressive terms. The ACF plot's decaying pattern suggests potential seasonality in the redemption data. I used outlier detection method to detect redemption/sales outliers and compared outlier’s mean with rest of the data before training models with clean data to reduce uncertainty. 

I started with simple seasonal decomposition (seasonal_decompose)as base model. Then I derived daily seasonal averages from historical data as the baseline forecasting method.

I decided to start with basic statistical models like ARIMA , and Exponential Smoothing as a baseline. My ADF test and PACF plot encouraged me to apply auto ARIMA model to deal with such stationary but seasonal uncertainty which peaked during summertime. ARIMA and ETS (Holt-Winters) models were tested but underperformed against the baseline model. They showed high error and poor generalization, indicating limited usefulness for complex seasonal patterns and weak trends in the data. I also Added Auto-ARIMA model (pmdarima) for automatic seasonal parameter selection. I integrated Facebook’s Prophet model included built-in holiday effects and yearly/weekly seasonality. I also considered multiplicative seasonality for better fit with changing seasonal amplitude. This model Include external factors such as holidays, promotions, economic indicators, and competitor activities.

Later I utilized advanced machine learning models such as Prophet, tree-based models- Random Forest, and Gradient Boosting for capturing detailed patterns/ trends. I also considered ensemble models that merge multiple machine learning for enhanced robustness which helps to smooth out the prediction. I also systematically tuned model parameters using grid search to maximize forecast accuracy for sales and redemption models. 

I Implemented and tested ensemble tree-based models: XGBoost and LightGBM with seasonal features and lagged variables. I regularized models using hyperparameter tuning to avoid overfitting. I also created a "typical seasonal pattern" by averaging predictions over multiple years per day-of-year. Performance is “better” with fewer folds because each fold’s test set is longer, often further in the future from the training period (more “stable” for the model).I experimented with a deep learning model (LSTM), but its performance was not satisfactory. I also explored different methods of time series cross-validation. I performed hyperparameter tuning for each tree based models. I also added sales lags and redemption lags in each respective models. Leaf-wise growth in LightGBM enables aggressive splits, potentially causing overfitting if unchecked.
 


